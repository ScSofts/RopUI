# RopHive Scheduling Layer: Theoretical Design

Before designing EventLoop, I did not think deeply about cross-platform behavior. After looking at Windows and MacOS, the original EventLoop design showed its weaknesses.

Windows and MacOS UI stacks involve the kernel. The interfaces they expose can be seen as wrappers of IO syscalls like Linux epoll/poll, but they are system-level APIs that cannot be split apart. Because of that, any IO syscall that might be inside those wrappers must be used through the system-provided API. They typically cannot be separated into a "real" IO syscall (for example, IOCP on Windows or kqueue on MacOS). These APIs also have limits. For instance, on Windows, `MsgWaitForMultipleObjectsEx` limits the number of handles to 64. This makes multi-threading for networking unavoidable. In practice the UI thread can handle some network work; having a dedicated networking thread may be wasteful. So we need a multi-threaded scheduler that is more complex than EventLoop.

I call this scheduler `RopHive`, a hive under Rop. `RopHive` aims to build an **IO-Computing hybrid-driven** multi-threaded scheduling system. It abstracts system-level UI threads as IO syscalls, and uses layered queues to resolve the tension between UI responsiveness and heavy computation.

## 1. RopHive

RopHive is the scheduling center. The design follows this pseudocode:

```cpp
RopHive hive(); // Create a hive

HiveWorker worker1(...); // Create a Worker bee
worker1.dosomething();

hive.attach(worker1);

hive.run();
```

Workers are a wrapper for thread resources. They have thread affinity. When binding workers, RopHive adjusts based on the number of workers. Eventually these workers bind to an internal thread pool (a worker pool). The actual binding to threads and run should happen in `run()`.

RopHive supports dynamically adding workers.

RopHive is a real scheduler. Externally, it accepts `RopTask` submissions. Tasks can be immediate (like `post` in EventLoop) or delayed (like `postDelay` in EventLoop). In a multi-threaded model, tasks can also be submitted to a specific worker.
`RopTask` is much more complex than in EventLoop. In EventLoop it is just a void function. Here it is far more complex.

RopHive can create dedicated compute workers. They simulate syscalls using multithreading primitives, similar to a regular thread pool.

RopTask is split into IO-intensive and compute-intensive tasks. Compute tasks are posted to a special queue and pulled in shared mode by compute workers. IO-intensive tasks are handled by other IO workers, and the main algorithm is work stealing. The former can be called `ComputeTask`, the latter `MicroTask`. Tasks are placed into `GlobalComputeTaskPool` and `GlobalMicroTaskPool` maintained by RopHive.

About `postDelay`: compute tasks are not allowed to be delayed. In debug, assert; in release, ignore.
`hive.postDelay` submits to the delay pool of a worker. If there is only one worker, submit to it. If there are two or more, use:

Logic:
    Randomly pick two IO workers (A and B).
    Compare their `timer_heap_.size()` (heap size).
    Send to the smaller heap (fewer tasks).
This requires workers to expose a counter for heap size.

When `hive.run()` is called, at least one worker must be attached. The first worker runs on the main thread.

The hive can detach any worker except the main-thread worker. It does not reclaim tasks from that worker. Workers can be created while running.

## 2. Worker (bee)

The smallest scheduling unit inside the hive. Each Worker binds to a system thread. Workers are equivalent containers. Each holds an `EventLoopCore`, and can register raw `EventSource` instances similarly to EventLoop. The `EventLoopCore` determines the worker's behavior.

`EventLoopCore` is the actual system wait core. On Windows, for example, you can have `RopHive::Windows::IOCPEventLoopCore` and `RopHive::Windows::Win32EventLoopCore`, which define what events can be registered and how `wait(timeout)` works.

There are special compute workers that might only have a condition-variable-based `EventLoopCore`.

Each worker has a private queue, a local double-ended queue for tasks that do not have to be handled by itself, and a `timer_heap` for local timed tasks. Because the private queue can receive `postDelayed` submissions from RopHive, it needs an `inbound_buffer` to accept tasks specific to that worker (it merges this buffer into `private_queue` during processing). Thread safety must be handled.

Workers provide a set of `post` methods. These `post` methods handle tasks generated by the current worker, usually created by callbacks of event sources. If a task must run on the current thread, call `postSelf` to push into `private_queue`. If the task is not thread-bound or is compute-heavy, call `RopHive::post` to submit to the global pool. If the task is not thread-bound but will access thread-local resources and can be faster on this thread, use `postToLocal` to put it into the double-ended queue so others can steal.

Among these queues, only `DQueue` has a maximum size. It should be set during worker initialization based on semantics. When `DQueue` is full, `postToLocal` becomes a `post` to the hive global pool, ensuring the global pool is processed and work stealing is bypassed under heavy load.

For timed tasks, workers should prefer `postSelfDelayed`, but they can also submit to RopHive to pick a worker.

A task should be able to obtain the worker id it runs on. The id is assigned and stored in the hive. A simple incrementing index is fine.

Workers must hold a wakeup event source. The registration is similar to EventLoop's watcher and is called `WorkerWatcher`. EventSources are platform-specific. Registration into the worker's EventLoopCore should check platform compatibility via an enum; if mismatched, drop and warn. In debug, assert.

Workers need a wakeup mechanism via `WakeupWorkerWatcher`. `WakeupWorkerWatcher` is a concrete `WorkerWatcher`. The worker owns this watcher and exposes a `wakeup` method; this is the only thread-safe way to wake the worker.

## 3. Hive rhythm: Worker run logic

Each `Worker::run()` loop follows a strict "energy harvest" order to avoid blocking system API messages:

1. **System messages first**: call `IEventLoopCore::runOnce(timeout)`. On Windows this corresponds to `PeekMessage`; on Linux it handles FD IO. Then call the raw callback handler in `IEventLoopCore` so those callbacks can post tasks via worker/hive semantics.
2. **Directed harvest (Private)**: take and clear `InboundBuffer`, merge it into `private_queue`, and handle cross-thread `postPrivate` (highest priority, usually state machine updates) and thread-bound tasks generated locally.
3. **Timers (Timer)**: run expired delayed tasks, same as EventLoop.
4. **Local digestion (Local DQueue Processing)**: the worker targets its `local_dqueue`. Using LIFO, it pops from the bottom. If tasks exist, it executes up to `N` tasks (a number to be determined, a segment function of DQueue size; this is a system hyperparameter). After execution it returns to step 1 to keep IO responsiveness. When `local_dqueue` is empty (bottom catches top), the worker enters step 5. Additionally, every `global_probe_interval` (tentatively 47) cycles that do not reach step 5, it tries step 5.
5. **Global harvest (Global Queue Harvesting)**: when local work is exhausted, the worker requests tasks from `GlobalMicroTaskPool`. To balance throughput and lock cost, it batch-pops (e.g. 16 tasks), fills its `local_dqueue`, and loops back to step 1 (practically skipping 6 and setting timeout=0 in 7 to make wakeup work). If the global pool is empty, go to step 6.
6. **Work stealing**: last resort. The worker randomly selects a neighbor worker from the hive, peeks its `local_dqueue`, and attempts CAS-based stealing. On success it moves tasks to its own queue. If it cannot steal from several neighbors, it goes to step 7.
7. **Deep sleep & wait**: step 7 is essentially a new loop iteration, same as step 1, but listed as the final fallback. The worker checks `timer_heap` to compute the nearest timeout. If no timers, timeout = -1. Then it calls `IEventLoopCore::runOnce(timeout)` to block in kernel mode. The thread sleeps until an FD event arrives, the timeout expires, or another thread wakes it via Waker (e.g., eventfd or PostThreadMessage). Then it restarts at step 1.

## 4. Other

The following post methods are described below (compute workers are not included):
|Scenario|Task Type|Recommended|Final Destination|Stealable|Wakeup Strategy|
|:-:|:-:|:-:|:-:|:-:|:-:|
|External random task|Any|hive.post|Global pool (Global Pool)|Yes|Wake one idle Worker only|
|External timed task|Micro|hive.postDelayed|A worker's timer_heap|No|Wake target Worker immediately|
|Cross-thread state update|Any|hive.postToWorker|Target private_queue|No|Wake target Worker immediately|
|Child task from Worker|Micro|worker.postToLocal|local_dqueue (or spill to Global)|Yes|Only wake if Worker is sleeping|
|Worker internal callback|Any|worker.postSelf|private_queue|No|Only wake if Worker is sleeping|

When `hive.post` is called inside a worker, the worker needs a buffer because each loop passes step 7. Before step 7 (the end of the loop), tasks placed by `post` are flushed to the global pool via `postBatch`, then the buffer is cleared.

Wake-one-idle-worker strategy: the hive maintains a `sleeping_io_workers` bitmap/list. If non-empty, wake one; if empty, do not wake (some worker is already running). Each worker writes its own sleep flag, the hive reads all. Flags are protected by `atomic<bool>`.

Other methods are flexible; `postBatch` is optional. The proxy post method that fills the worker buffer and the batch upload between steps 6 and 7 (step 6.5) should be implemented; it is not mandatory but significantly improves throughput under heavy load. A switch can control whether a worker enables this optimization.

Multi-thread race details are decided when designing concrete classes.
